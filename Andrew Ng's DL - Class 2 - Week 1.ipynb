{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Training set, validation set and test set\n",
    "\n",
    "- **Training set**: tune the model parameters. 60% of whole data set.\n",
    "- **Validation set**: i.e., development set. Select hyper-parameters (i.e., different models) to avoid overfitting. 20% of whole data set.\n",
    "- **Test set**: report the performance of trained model. 20% of whole data set.\n",
    "\n",
    "**Note**:\n",
    "- **Validation set** is not exactly needed.\n",
    "- **Validation set** and **Test set** should follow the **same distribution**.\n",
    "\n",
    "reference:  \n",
    "https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Bias and variance tradeoff\n",
    "- **Total Error = Bias + Variance + Irreducible Error**\n",
    "- Bias: under-fitting\n",
    "- Variance: over-fitting\n",
    "\n",
    "In deep learning, bias and variance tradeoff is less discussed.\n",
    "\n",
    "### Bias and variance illustration\n",
    "Clicking for illustration: https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/Andrew_Ng_images/Class_2_week_1/Bias_variance.png\n",
    "\n",
    "reference:  \n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n",
    "\n",
    "### The relationship between model complexity and error\n",
    "\n",
    "Clicking for illustration: https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/Andrew_Ng_images/Class_2_week_1/Error_complexity.png\n",
    "\n",
    "#### How to detect high bias ?\n",
    "- Training error is high.\n",
    "- Validation error has **similar magnitude** to training error.\n",
    "\n",
    "#### How to detect high variance ?\n",
    "- Training error is low.\n",
    "- Validation error is **very high**.\n",
    "\n",
    "### How to solve the problem ?\n",
    "Clicking for illustration: https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/Andrew_Ng_images/Class_2_week_1/Under_over_fitting_solution.png\n",
    "\n",
    "reference:  \n",
    "http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Reduce over-fitting\n",
    "### 1.3.1 Regularization\n",
    "\n",
    "**Forward propagation**: \n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} $$\n",
    "\n",
    "To calculate $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$  , use :\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "```\n",
    "\n",
    "**Backward propagation**: \n",
    "only concern dW1, dW2. \n",
    "For each, add the regularization term's gradient ($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$).\n",
    "\n",
    "**Note**: \n",
    "- `1.0 / m * lambd / 2.0`. `lambd` is a hyperparameter.\n",
    "- L2 regularization makes the decision boundary smoother.\n",
    "- Weights end up smaller (\"weight decay\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward propagation computes cost\n",
    "# suppose that there are sigler hidden layer neural network\n",
    "# W1 and W2 are parameters for input X and hidden neurons\n",
    "L2_regularization_cost = 1.0 / m * lambd / 2.0 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "cost = cross_entropy_cost + L2_regularization_cost\n",
    "\n",
    "\n",
    "# back propagation comptutes gradients\n",
    "dZ2 = A2 - Y # the cross_entropy loss\n",
    "dW2 = 1./m * np.dot(dZ2, A1.T) + W2 * lambd / m\n",
    "db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "dA1 = np.dot(W2.T, dZ2)\n",
    "dZ1 = np.multiply(dA1, np.int64(A1 > 0)) # relu activation\n",
    "dW1 = 1./m * np.dot(dZ1, X.T) + W1 * lambd / m\n",
    "db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Dropout\n",
    "- In each iteration, it randomly shuts down (= set to zero) some neurons with probability $1 - keep\\_prob$.\n",
    "- The dropped neurons don't contribute to the training in both the **forward and backward propagations** of the iteration.\n",
    "- The idea is that at each iteration, you train a **different model** that uses only a subset of your neurons. The role of one specific neuron is reduced because that it might be shut down at any time. \n",
    "- **Inverted dropout**: Divide $A^{[1]}$ by `keep_prob`. Assure that the result of the cost will still have the **same expected value as without drop-out**.\n",
    "- Dropout should only be used in **training**, not testing.\n",
    "\n",
    "Note: initialize the random numbers follwing the uniform distribution, i.e., `np.random.rand(a, b)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward propagation computes cost\n",
    "Z1 = np.dot(W1, X) + b1\n",
    "A1 = relu(Z1)\n",
    "D1 = np.random.rand(A1.shape[0], A1.shape[1])\n",
    "D1 = np.int64(D1 < keep_prob)\n",
    "A1 = np.multiply(A1, D1)\n",
    "# inverted dropout: scale the value of neurons that haven't been shut down\n",
    "A1 = A1 / keep_prob\n",
    "\n",
    "Z2 = np.dot(W2, A1) + b2\n",
    "A2 = relu(Z2)\n",
    "D2 = np.random.rand(A2.shape[0], A2.shape[1])\n",
    "D2 = np.int64(D2 < keep_prob)\n",
    "A2 = np.multiply(A2, D2)\n",
    "A2 = A2 / keep_prob\n",
    "Z3 = np.dot(W3, A2) + b3\n",
    "A3 = sigmoid(Z3)\n",
    "    \n",
    "\n",
    "# back propagation comptutes gradients\n",
    "dZ3 = A3 - Y\n",
    "dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "dA2 = np.dot(W3.T, dZ3)\n",
    "\n",
    "# Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "dA2 = np.multiply(dA2, D2)\n",
    "# Step 2: Scale the value of neurons that haven't been shut down\n",
    "dA2 = dA2 / keep_prob\n",
    "\n",
    "dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "dA1 = np.dot(W2.T, dZ2)\n",
    "\n",
    "# Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "dA1 = np.multiply(dA1, D1)\n",
    "# Step 2: Scale the value of neurons that haven't been shut down\n",
    "dA1 = dA1 / keep_prob\n",
    "\n",
    "dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.4 Accelerate deep network training\n",
    "- Poor initialization can lead to **vanishing/exploding gradients**, which also slows down the optimization.\n",
    "- The goal is speeding up the convergence of gradient descent.\n",
    "\n",
    "### 1.4.1: input data normalization\n",
    "### 1.4.2: weights initialization\n",
    "- The weights $W^{[l]}$ should be initialized randomly to **break symmetry**. \n",
    "- It is okay to initialize the biases $b^{[l]}$ to **zeros**. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly. \n",
    "- **He** initialization: scaling factor for the weights $W^{[l]}$ is $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$. It is recommended for layers with a **ReLU** activation.\n",
    "- **Xavier** initialization: scaling factor for the weights $W^{[l]}$ is $\\sqrt{\\frac{1}{\\text{dimension of the previous layer}}}$.\n",
    "\n",
    "The basic principle about **He** and **Xavier** initialization is let $Var(Y) = Var(X)$. Two references [a](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) and [b](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/) explains the principle in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers     \n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2.0/layers_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow implementation\n",
    "W = tf.get_variable('w', shape=[a, b], initializer=tf.contrib.layers.xavier_initializer())\n",
    "# see https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
