{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mini-batch gradient descent\n",
    "- **Epoch**: a single pass through the whole training set.\n",
    "\n",
    "- Variants\n",
    "    - **(Batch) gradient descent**: whole training set at each training step.\n",
    "    - **Mini-batch gradient descent**: `batch_size` samples at each training step. `batch_size` is a **hyper-parameter** (which should be finetuned) and commonly used value is 32, 64, 128, 256, 512.\n",
    "    - **Stochastic gradient descent**: one sample at each training step.\n",
    "\n",
    "- Rule of thumb\n",
    "    - When the size of training set is less than 2000, use batch gradient descent.\n",
    "    - Otherwise, use mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2 Variants of gradient descent\n",
    "\n",
    "References:\n",
    "http://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
