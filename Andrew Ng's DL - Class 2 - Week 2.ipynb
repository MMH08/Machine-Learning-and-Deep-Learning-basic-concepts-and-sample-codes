{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mini-batch gradient descent\n",
    "- **Epoch**: a single pass through the whole training set.\n",
    "- **Saddle point**: saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.\n",
    "\n",
    "\n",
    "- Variants of gradient descent\n",
    "    - **(Batch) gradient descent**: whole training set at each training step. Batch gradient descent is guaranteed to converge to the global minimum for convex cost function and to a local minimum  for non-convex cost function [1].\n",
    "    - **Mini-batch gradient descent**: `batch_size` samples at each training step. `batch_size` is a **hyper-parameter** (which should be finetuned) and commonly used value is 32, 64, 128, 256, 512.\n",
    "    - **Stochastic gradient descent**: one sample at each training step. With the help of slowly decreased learning rate, stochastic gradient descent has the same convergence trendency as the batch gradient descent [1].\n",
    "\n",
    "- Rule of thumb\n",
    "    - When the size of training set is less than 2000, use batch gradient descent.\n",
    "    - Otherwise, use mini-batch gradient descent.\n",
    "    \n",
    "References:\n",
    "[1] http://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Exponential weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3 Variants of gradient descent optimization algorithm\n",
    "- Momentum: use exponential weighted average of gradients to update parameter. The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.\n",
    "\n",
    "- RMSprop\n",
    "- Adam (Adaptive moment estimation)\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "http://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
